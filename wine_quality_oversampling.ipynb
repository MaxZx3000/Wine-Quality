{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Preprocessing (Oversampling)\n",
    "\n",
    "Made by: Anthony Kevin Oktavius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import klib\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import shap\n",
    "import hyperopt\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.stats import zscore\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I'm going to look at Wine Quality Oversampling Technique. In our case, we will use 3 types of oversampling technique, which are:\n",
    "\n",
    "* Random Over Sampling\n",
    "* SMOTE\n",
    "* ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Over Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Random Oversampling](https://miro.medium.com/max/375/1*aKJJOozIlVVH1gT-4rYy4w.png)\n",
    "\n",
    "Image Source: https://miro.medium.com/max/375/1*aKJJOozIlVVH1gT-4rYy4w.png\n",
    "\n",
    "![Random Oversampling Plot](https://imbalanced-learn.org/stable/_images/sphx_glr_plot_comparison_over_sampling_002.png)\n",
    "\n",
    "Image Source: https://imbalanced-learn.org/stable/_images/sphx_glr_plot_comparison_over_sampling_002.png\n",
    "\n",
    "Random Oversampling includes duplicating examples from minority classes. The duplicated minority examples will then be added to the original dataset, so it allows the data to be selected again.\n",
    "\n",
    "This technique is effective when we'd like to normalize skewed distributions (mean and standard deviation are not 0). However, in some cases, doing this technique can cause the algorithm to overfit the minority class, since the same data row can be duplicated again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SMOTE](https://miro.medium.com/max/1400/1*CG712DHQL_ZMS3gAEGd_5w.jpeg)\n",
    "\n",
    "Image Source: https://miro.medium.com/max/1400/1*CG712DHQL_ZMS3gAEGd_5w.jpeg\n",
    "\n",
    "Unlike Random Over Sampling where we copy the values from the actual dataset, SMOTE creates new synthetic observations/ new data points.\n",
    "\n",
    "Here are the summary steps on SMOTE Process:\n",
    "* Plot the data points.\n",
    "* Identify feature vector and its nearest neighbour. In this case, we're using K-Nearest Neighbors.\n",
    "* Take the linear distance difference between the two\n",
    "* Multiply the difference with a random number between 0 and 1\n",
    "* Identify a new point on the line segment by adding the random number to feature vector\n",
    "* Repeat the process for identified feature vectors, until our defined data points that weâ€™d like to create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison between ADASYN and SMOTE can be seen in this research paper: https://www.diva-portal.org/smash/get/diva2:1519153/FULLTEXT01.pdf.\n",
    "\n",
    "ADASYN stands for Adaptive Synthetic Algorithm.\n",
    "\n",
    "ADASYN works similar to SMOTE. Below are some characteristics on ADASYN:\n",
    "\n",
    "* ADASYN algorithm works on providing more synthetic data on harder-to-learn than easier-to-learn data.\n",
    "* ADASYN generates synthetic observations along a straight line between a minority class observation and k-nearest minority class neighbor.\n",
    "\n",
    "Here's how ADASYN works:\n",
    "\n",
    "* Calculate the degree of class imbalance: d = $\\frac{m_s}{m_l}$\n",
    "\n",
    "  Where d will be between 0 and 1.\n",
    "* If d < $d_{th}$, where th is the defined threshold ratio: \n",
    "  \n",
    "  * Calculate the number of synthetic data examples that needs to be generated from minority class.\n",
    "\n",
    "  $G = (m_{l} - m_{s}) * \\beta $\n",
    "\n",
    "  Where $\\beta$ can be either 0 or 1. $\\beta$ represents the desired balance level after synthetic data generation (e.g. $\\beta$ = 1, then the data is fully balanced after synthetic data generation).\n",
    "\n",
    "  * For each example $x_i$  $\\epsilon$  minority class, find K nearest neighbors based on eucledian distance in N dimensional space.\n",
    "\n",
    "  Calculate the ratio ${r_i}$, defined as:\n",
    "\n",
    "  ${r_i} = $ ${\\Delta}_i / K$\n",
    "\n",
    "  Where: ${\\Delta}_i$ is the number of examples in the K nearest neighbors of majority class. ${r_i}$ is equal to 0 or 1.\n",
    "\n",
    "  * Normalize ${r_i}$ according to $\\hat{r_i}$ = ${r_i}$ / $\\sum_{i = 1}^{m_s} r_i$, so $\\hat{r_i}$ is a density distribution (sum of $\\hat{r_i}$ = 1).\n",
    "\n",
    "  * Calculate the number of synthetic data examples that needs to be generated for minority example ${x_i}$.\n",
    "\n",
    "    ${g_i}$ = $\\hat{r_i}$ * G\n",
    "\n",
    "    Where G is the total number of synthetic data examples that needs to be generated for minority class, as defined by equation (2).\n",
    "\n",
    "  * For each minority classes in ${x_i}$, generate ${g_i}$ synthetic data examples.\n",
    "    Looping from 1 to ${g_i}$:\n",
    "\n",
    "      * Randomly choose 1 minority data example (${x_{xi}}$) from K nearest neighbots for data ${x_i}$.\n",
    "      \n",
    "      * Generate synthetic data example:\n",
    "        ${s_i}$ = ${x_i} + (x_{zi} - x_i) * \\lambda$\n",
    "        \n",
    "        x_{zi} - x_i represents different vector in dimensional spaces and \\lambda is a random number either 0 or 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
